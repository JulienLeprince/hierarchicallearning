{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a3c9e56",
   "metadata": {},
   "source": [
    "# Forecasting Trees\n",
    "\n",
    "This notebook intends on showcasing how the `TreeRegressor` class may be employed to predict hierarchical time series, based on the developed `TreeClass` objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0bb2dd49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading utils...\n",
      "done!\n"
     ]
    }
   ],
   "source": [
    "# Standard Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# Tree package\n",
    "import hierarchicalearning.TreeClass as treepkg\n",
    "import hierarchicalearning.HPL_utils as utils\n",
    "import hierarchicalearning.TreeRegressor as regpkg\n",
    "\n",
    "# Defining a simple (A, (B, C, D)) hierarchy\n",
    "leaves = ['B', 'C', 'D']\n",
    "periods = 100\n",
    "random_data = np.random.rand(periods, 3)\n",
    "tidx = pd.date_range('2022-01-01', periods=periods, freq='H')\n",
    "df = pd.DataFrame(random_data, columns=leaves, index=tidx)\n",
    "\n",
    "df_tree = dict()\n",
    "for leaf_id in leaves:\n",
    "    df_tree[leaf_id] = df.loc[:, [leaf_id]]\n",
    "    df_tree[leaf_id].rename(columns={leaf_id: 'node_timeseries1'}, inplace=True)\n",
    "\n",
    "# Creating a simple spatial tree\n",
    "root = treepkg.Node(id='A',\n",
    "                    parent=None,\n",
    "                    children=leaves)\n",
    "nodes = {root}\n",
    "for leaf_id in leaves:\n",
    "    leaf = treepkg.Node(id=leaf_id,\n",
    "                        parent='A',\n",
    "                        children=None)\n",
    "    nodes.add(leaf)\n",
    "tree = treepkg.Tree(nodes, dimension='spatial')\n",
    "tree.create_spatial_hierarchy(df_tree, columns2aggr=['node_timeseries1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c2c21c",
   "metadata": {},
   "source": [
    "### Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99b8975c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No partial auto-correlation was found respecting the defined threshold. Threshold is re-adapted to highest PAC value:  0.20517570642764643\n",
      "No partial auto-correlation was found respecting the defined threshold. Threshold is re-adapted to highest PAC value:  0.21278353230183344\n",
      "No partial auto-correlation was found respecting the defined threshold. Threshold is re-adapted to highest PAC value:  0.20362413079810351\n",
      "No partial auto-correlation was found respecting the defined threshold. Threshold is re-adapted to highest PAC value:  0.22054681064734785\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'B': {'node_timeseries1': ['P0DT18H0M0S']},\n",
       " 'C': {'node_timeseries1': ['P0DT7H0M0S']},\n",
       " 'D': {'node_timeseries1': ['P0DT18H0M0S']},\n",
       " 'A': {'node_timeseries1': ['P0DT12H0M0S']}}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import statsmodels.api as sm\n",
    "feature_toshift = {}\n",
    "target = 'node_timeseries1'\n",
    "threshold = 0.25\n",
    "\n",
    "for node in tree.df:\n",
    "    pac_largest_timedeltas = utils.get_pac_largest_timedeltas(tree.df[node][target], n=3, threshold=threshold)\n",
    "    feature_toshift[node] = {target: pac_largest_timedeltas}\n",
    "\n",
    "feature_toshift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ea113ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'node_timeseries1'\n",
    "### Defining target, features and feature shifts\n",
    "target = [target]\n",
    "features = dict()\n",
    "for n in tree.leaves_label:\n",
    "    features[n] = list(tree.df[n].columns)\n",
    "    features[n].remove(target[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc8a9ee",
   "metadata": {},
   "source": [
    "## Hierarchical Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eec955a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hierarchical forecasting methods:\n",
    "hierarchical_forecasting_methods = ['multitask', 'OLS', 'BU', 'STR', 'hVAR', 'sVAR', 'COV', 'kCOV']\n",
    "hierarchical_forecasting_method = 'multitask'\n",
    "# reconciliation methods: 'None', 'OLS', 'BU', 'STR', 'hVAR', 'sVAR', 'COV', 'kCOV'\n",
    "hierarchical_reconciliation_method = 'None'\n",
    "\n",
    "hreg = regpkg.H_Regressor(tree, target_in=target, features_in=features,\n",
    "                          forecasting_method=hierarchical_forecasting_method,\n",
    "                          reconciliation_method=hierarchical_reconciliation_method,\n",
    "                          alpha=0.75)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee330afc",
   "metadata": {},
   "source": [
    "Here we redefine the features of the regressor with the method `feature_engineering_wrapper`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1cb3984d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "engineering features of node:  B\n",
      "engineering features of node:  C\n",
      "engineering features of node:  D\n"
     ]
    }
   ],
   "source": [
    "hreg.feature_engineering_wrapper(feature_toshift)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "07214431",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting and scaling training/testing sets, iteration  0  over  10\n",
      "Splitting and scaling training/testing sets, iteration  1  over  10\n",
      "Splitting and scaling training/testing sets, iteration  2  over  10\n",
      "Splitting and scaling training/testing sets, iteration  3  over  10\n",
      "Splitting and scaling training/testing sets, iteration  4  over  10\n",
      "Splitting and scaling training/testing sets, iteration  5  over  10\n",
      "Splitting and scaling training/testing sets, iteration  6  over  10\n",
      "Splitting and scaling training/testing sets, iteration  7  over  10\n",
      "Splitting and scaling training/testing sets, iteration  8  over  10\n",
      "Splitting and scaling training/testing sets, iteration  9  over  10\n"
     ]
    }
   ],
   "source": [
    "# Obtain training and testing sets\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "nb_splits = 10\n",
    "X_train, X_test, y_train, y_test, y_test_allflats = hreg.split_scaled(nb_splits=nb_splits, toscale=True,\n",
    "                                                                      scaler=StandardScaler(),\n",
    "                                                                      only_leaves_X=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9249e63e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "WARNING:tensorflow:5 out of the last 8 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000001E4DEF28790> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "4\n",
      "WARNING:tensorflow:6 out of the last 10 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000001E4DD48B790> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "# Create the network\n",
    "hreg.create_ANN_network()\n",
    "\n",
    "# Outputs to save\n",
    "y_forecasted = []\n",
    "\n",
    "## Train the model\n",
    "for i in range(nb_splits):\n",
    "    print(i)\n",
    "    # For the first round, we initialise the G matrix as an OLS method - since we do not have prediction error estimates yet\n",
    "    hreg.scaler_attribute_update(divider=np.transpose(hreg.target_scalers['divider'][i]),\n",
    "                                    shift=np.transpose(hreg.target_scalers['shift'][i]))\n",
    "    if i == 0:\n",
    "        hreg.coherency_constraint(y_diff=None)\n",
    "        hreg.yhat_initialization(columns_in=target)\n",
    "    # We train the DANN\n",
    "    hreg.regressor.fit(X_train[i],\n",
    "                        y_train[i],\n",
    "                        epochs=500,\n",
    "                        batch_size=24,\n",
    "                        verbose=0,\n",
    "                        shuffle=False)\n",
    "\n",
    "    # We obtain the y_hat prediction over the test set\n",
    "    y_hat = hreg.regressor.predict(X_test[i])\n",
    "    # Save y_hat to a hierarchical df format\n",
    "    y_hat_unscaled = hreg.inverse_scale_y(y_hat)\n",
    "    hreg.yhat = hreg.tree.reshape_flat2tree(flat_all_inputs=y_test_allflats[i], flat_data_in=y_hat_unscaled,\n",
    "                                            tree_df_in=hreg.yhat)\n",
    "\n",
    "    ## Update the coherency constraint in the loss function\n",
    "    y_diff = np.transpose(y_test[i]) - np.transpose(y_hat)\n",
    "    hreg.coherency_constraint(y_diff=y_diff)\n",
    "    # We recompile the regressor to update the new loss function\n",
    "    # see - https://stackoverflow.com/questions/60996892/how-to-replace-loss-function-during-training-tensorflow-keras\n",
    "    loss_fct = hreg.coherency_loss_function_mse if hreg.hierarchical_forecasting_method != 'multitask' \\\n",
    "        else hreg.independent_loss_function_mse\n",
    "    hreg.regressor.compile(loss=loss_fct,\n",
    "                           optimizer='Adam',\n",
    "                           metrics=['mae', 'mse'])\n",
    "\n",
    "\n",
    "    # Inverse scaling y vectors for reconciliation\n",
    "    y_test_unscaled = hreg.inverse_scale_y(y_test[i])\n",
    "    y_hatrain = hreg.regressor.predict(X_train[i])\n",
    "    y_hatrain_unscaled = hreg.inverse_scale_y(y_hatrain)\n",
    "    y_train_unscaled = hreg.inverse_scale_y(y_train[i])\n",
    "    y_diff = np.transpose(y_train_unscaled) - np.transpose(y_hatrain_unscaled)\n",
    "\n",
    "    # A-posteriori hard-constrained reconciliation\n",
    "    y_tild = hreg.hard_ctr_reconciliation(y_diff=y_diff, y_hat=y_hat_unscaled,\n",
    "                                          method=hierarchical_reconciliation_method)\n",
    "    y_forecasted.append(y_tild)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('tensorflow_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "2fe71c061d7947f6cb537a314e941db93d1e10209bb64f506b9d2feb02231d99"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
